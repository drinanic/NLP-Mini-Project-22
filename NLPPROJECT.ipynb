{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I imported in all of the important libraries. Then, I loaded in a compiled version of all the data from season 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_csv = pandas.read_csv(r'C:\\Users\\alexn\\OneDrive\\Desktop\\NLP-Mini-Project-22\\scripts\\merged-csv-files.csv', encoding='utf-8') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I added some of the most common screenplay (non dialogue) words into the stopwords so that only the dialogue is being analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "nlp.Defaults.stop_words |= {\"d\",\"ll\",\"m\",\"re\",\"s\",\"ve\", \"t\", \"oh\", \"uh\", \"na\", \"okay\",\n",
    "                           \"didn\",\"don\",\"gon\",\"j\",\"hm\",\"um\",\"dr\",\"room\",\"int\", \"ext\", \n",
    "                           \"cut\", \"day\", \"night\", \"theme\", \"tune\",\"music\", \"ends\",\"view\",\n",
    "                            \"closeup\", 'freshly', 'squeezed', 'fade'}\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I analysed the top 10 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(words,title,color=\"#114d1e\"):\n",
    "    counts = {}\n",
    "    for i in range(len(words)):\n",
    "        counts[words[i][0]] = words[i][1]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.barh(range(len(counts)), list(counts.values()), color=color, align=\"center\")\n",
    "    plt.yticks(range(len(counts)), list(counts.keys()), fontsize=12)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_ngrams(ngrams,title,color=\"#7a2822\"):\n",
    "    counts = {}\n",
    "    for i in range(len(ngrams)):\n",
    "        counts[\" \".join(ngrams[i][0])] = ngrams[i][1]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.barh(range(len(counts)), list(counts.values()), color=color,align=\"center\")\n",
    "    plt.yticks(range(len(counts)), list(counts.keys()), fontsize=12)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    all_words = nltk.tokenize.word_tokenize(text.lower())\n",
    "all_words_no_stop = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords)\n",
    "plot_words(all_words_no_stop.most_common(10), \"Most Frequent Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the top ten words are the names of the central characters: 'cooper', 'sheriff', 'truman', 'laura', 'bobby', 'donna', 'audrey', 'james', 'leo' 'ed'. \n",
    "\n",
    "Then, I analysed the top 10 most frquent bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = nltk.FreqDist(nltk.bigrams(w.lower() for w in all_words if w not in stopwords))\n",
    "plot_ngrams(bigram.most_common(10), \"Most Frequent Bigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were: 'sheriff truman', 'big ed', 'twin peaks', 'laura palmer', 'agent cooper', 'peaks sheriff', 'sheriff department', 'truman cooper', 'log lady', 'cooper sheriff'.\n",
    "\n",
    "Then, I analysed the top 10 most frequent trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = nltk.FreqDist(nltk.trigrams(w.lower() for w in all_words if w not in stopwords))\n",
    "plot_ngrams(trigrams.most_common(10), \"Most Frequent Trigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were: 'twin peaks sheriff', 'peaks sheriff department', 'sheriff truman cooper', 'cooper sheriff truman', 'blue pine lodge', 'great northern hotel', 'love twin peaks', 'double r diner', 'calhoun memorial hospital', 'palmer house sarah'. \n",
    "\n",
    "As all of these were name based, I removed the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\n",
    "'Dale Cooper',\n",
    "'Sheriff Harry Truman',\n",
    "'Shelly Johnson',\n",
    "'Bobby Briggs',\n",
    "'Benjamin Horne',\n",
    "'Donna Hayward',\n",
    "'Audrey Horne',\n",
    "'Will Hayward',\n",
    "'Norma Jennings',\n",
    "'James Hurley',\n",
    "'Ed Hurley',\n",
    "'Pete Martell',\n",
    "'Leland Palmer',\n",
    "'Josie Packard',\n",
    "'Catherine Martell',\n",
    "'Lucy Moran',\n",
    "'Laura Palmer',\n",
    "'Lawrence Jacoby',\n",
    "'Leo Johnson',\n",
    "'Eileen Hayward',\n",
    "'Andy Brennan',\n",
    "'Mike Nelson',\n",
    "'Tommy Hawk Hill'\n",
    "'Sarah Palmer',\n",
    "'Jacques Renault',\n",
    "'Windom Earle',\n",
    "'Ronette Pulaski',\n",
    "'Phillip Jeffries',\n",
    "'Albert Rosenfield',\n",
    "'Teresa Banks',\n",
    "'Annie Blackburn',\n",
    "'Chester Desmond',\n",
    "'Gordon Cole',\n",
    "'Carl Rodd',\n",
    "'Sam Stanley',\n",
    "'Harold Smith'\n",
    "]\n",
    "\n",
    "# unique names only\n",
    "names = set(\" \".join(set(characters)).lower().split())\n",
    "\n",
    "nlp.Defaults.stop_words |= names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_names = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords)\n",
    "plot_words(no_names.most_common(10), \"Top 10 Frequent Words Except Names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words are now: 'big', 'walks', 'know', 'peaks', 'twin', 'house', 'like', 'looks', 'door', 'man'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_names_bigram = nltk.FreqDist(nltk.bigrams(w.lower() for w in all_words if w not in stopwords))\n",
    "plot_ngrams(no_names_bigram.most_common(10), \"Top 10 Frequent Bigrams Except Names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent bigrams are now: 'twin peaks', 'peaks department', 'log lady', 'great northern', 'blue pine', 'pine lodge', 'northern hotel', 'bookhouse boys', 'love twin', 'double r'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_names_trigram = nltk.FreqDist(nltk.trigrams(w.lower() for w in all_words if w not in stopwords))\n",
    "plot_ngrams(no_names_trigram.most_common(10), \"Top 10 frequent trigrams except for names\", \"#2b2e2b\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent trigrams are now: 'twin peaks department', 'blue pine lodge', 'great northern hotel', 'love twin peaks', 'double r diner', 'calhoun memorial hospital', 'timber falls motel', 'dance dream man', 'life twin peaks', 'big gas farm'. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e245b6e8656557d201be73fbc1d00f8d35efdf4ab03c4356d8952ce09f2d82d2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
